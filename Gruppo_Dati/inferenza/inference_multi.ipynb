{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# if inference_results directory does not exist, create it\n",
    "if not os.path.exists(\"inference_results\"):\n",
    "    os.makedirs(\"inference_results\")\n",
    "    \n",
    "from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Image\n",
    "from reportlab.lib.styles import getSampleStyleSheet\n",
    "from reportlab.lib.pagesizes import A4\n",
    "from reportlab.lib.units import cm\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.executable)\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# LOAD MODELS AND ENCODERS\n",
    "# ============================================================================\n",
    "DATA_DIR = \"data\"\n",
    "\n",
    "with open(os.path.join(DATA_DIR, \"model_categoria.pkl\"), \"rb\") as f:\n",
    "    model_categoria = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(DATA_DIR, \"model_classe.pkl\"), \"rb\") as f:\n",
    "    model_classe = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(DATA_DIR, \"label_encoders_final.pkl\"), \"rb\") as f:\n",
    "    label_encoders = pickle.load(f)\n",
    "\n",
    "le_categoria = label_encoders[\"CATEGORIA\"]\n",
    "le_classe = label_encoders[\"CLASSE\"]\n",
    "\n",
    "df_model_reference = pd.read_csv(os.path.join(DATA_DIR, \"df_model.csv\"), low_memory=False)\n",
    "\n",
    "print(\"Models and encoders loaded successfully\")\n",
    "\n",
    "# Get expected features\n",
    "expected_features_cat = model_categoria.feature_names_in_\n",
    "expected_features_classe = model_classe.feature_names_in_\n",
    "\n",
    "print(f\"CATEGORIA model features: {len(expected_features_cat)}\")\n",
    "print(f\"CLASSE model features: {len(expected_features_classe)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# LOAD INPUT\n",
    "# ============================================================================\n",
    "df_input = pd.read_csv(\"sample_input.csv\")\n",
    "print(f\"Input data loaded: {df_input.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# PREPROCESSING\n",
    "# ============================================================================\n",
    "df_processed = df_input.copy()\n",
    "\n",
    "# Add missing columns for CATEGORIA\n",
    "for col in expected_features_cat:\n",
    "    if col not in df_processed.columns:\n",
    "        if col in df_model_reference.columns:\n",
    "            if pd.api.types.is_numeric_dtype(df_model_reference[col]):\n",
    "                df_processed[col] = df_model_reference[col].median()\n",
    "            else:\n",
    "                df_processed[col] = df_model_reference[col].mode()[0] if len(df_model_reference[col].mode()) > 0 else 0\n",
    "        else:\n",
    "            df_processed[col] = 0\n",
    "\n",
    "# Process numeric columns\n",
    "for col in expected_features_cat:\n",
    "    if col in df_model_reference.columns:\n",
    "        if pd.api.types.is_numeric_dtype(df_model_reference[col]):\n",
    "            df_processed[col] = pd.to_numeric(df_processed[col], errors='coerce')\n",
    "            df_processed[col] = df_processed[col].fillna(df_model_reference[col].median())\n",
    "\n",
    "# Process categorical columns\n",
    "for col, le in label_encoders.items():\n",
    "    if col in expected_features_cat and col != 'CATEGORIA' and col != 'CLASSE':\n",
    "        most_frequent = le.classes_[0]\n",
    "        df_processed[col] = df_processed[col].fillna(most_frequent).astype(str)\n",
    "        df_processed[col] = df_processed[col].apply(lambda x: x if x in le.classes_ else most_frequent)\n",
    "        df_processed[col] = le.transform(df_processed[col])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# STEP 1: PREDICT CATEGORIA\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 1: CATEGORIA PREDICTION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "X_cat = df_processed[expected_features_cat]\n",
    "pred_cat_encoded = model_categoria.predict(X_cat)\n",
    "# Convert to int to avoid dtype casting errors\n",
    "pred_cat_encoded = pred_cat_encoded.astype(int)\n",
    "pred_cat = le_categoria.inverse_transform(pred_cat_encoded)\n",
    "proba_cat = model_categoria.predict_proba(X_cat)\n",
    "\n",
    "print(f\"\\nTop 3 CATEGORIA predictions:\")\n",
    "top3_cat_indices = np.argsort(proba_cat[0])[-3:][::-1]\n",
    "for i, idx in enumerate(top3_cat_indices, 1):\n",
    "    cat_name = le_categoria.classes_[idx]\n",
    "    confidence = proba_cat[0][idx]\n",
    "    print(f\"  {i}. {cat_name}: {confidence:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# SHAP EXPLAINABILITY - CATEGORIA\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SHAP ANALYSIS - CATEGORIA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"Computing SHAP values...\")\n",
    "explainer_cat = shap.TreeExplainer(model_categoria)\n",
    "shap_values_cat = explainer_cat.shap_values(X_cat)\n",
    "\n",
    "# Extract values for predicted class\n",
    "if isinstance(shap_values_cat, np.ndarray) and shap_values_cat.ndim == 3:\n",
    "    shap_values_cat_pred = shap_values_cat[0, :, pred_cat_encoded[0]]\n",
    "    base_value = explainer_cat.expected_value[pred_cat_encoded[0]]\n",
    "elif isinstance(shap_values_cat, list):\n",
    "    shap_values_cat_pred = shap_values_cat[pred_cat_encoded[0]][0]\n",
    "    base_value = explainer_cat.expected_value[pred_cat_encoded[0]]\n",
    "else:\n",
    "    shap_values_cat_pred = shap_values_cat[0]\n",
    "    base_value = explainer_cat.expected_value\n",
    "\n",
    "# Top 30 most influential features\n",
    "shap_importance_cat = pd.DataFrame({\n",
    "    'feature': expected_features_cat,\n",
    "    'shap_value': shap_values_cat_pred,\n",
    "    'feature_value': X_cat.iloc[0].values\n",
    "})\n",
    "shap_importance_cat['abs_shap'] = shap_importance_cat['shap_value'].abs()\n",
    "shap_top30 = shap_importance_cat.sort_values('abs_shap', ascending=False).head(30)\n",
    "\n",
    "# Top 10 features that increased probability (positive SHAP)\n",
    "shap_positive = shap_importance_cat[shap_importance_cat['shap_value'] > 0].sort_values('shap_value', ascending=False).head(10)\n",
    "\n",
    "# Top 10 features that decreased probability (negative SHAP)\n",
    "shap_negative = shap_importance_cat[shap_importance_cat['shap_value'] < 0].sort_values('shap_value', ascending=True).head(10)\n",
    "\n",
    "print(f\"\\nTop 30 most influential features for CATEGORIA={pred_cat[0]}:\")\n",
    "print(\"-\" * 70)\n",
    "for idx, row in shap_top30.iterrows():\n",
    "    direction = \"positive\" if row['shap_value'] > 0 else \"negative\"\n",
    "    print(f\"  {row['feature']}: {row['feature_value']:.2f} | SHAP: {row['shap_value']:+.4f} ({direction})\")\n",
    "\n",
    "print(f\"\\nTop 10 features that INCREASED probability:\")\n",
    "print(\"-\" * 70)\n",
    "for idx, row in shap_positive.iterrows():\n",
    "    print(f\"  {row['feature']}: {row['feature_value']:.2f} | SHAP: {row['shap_value']:+.4f}\")\n",
    "\n",
    "print(f\"\\nTop 10 features that DECREASED probability:\")\n",
    "print(\"-\" * 70)\n",
    "for idx, row in shap_negative.iterrows():\n",
    "    print(f\"  {row['feature']}: {row['feature_value']:.2f} | SHAP: {row['shap_value']:+.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# STEP 2: PREDICT CLASSE\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 2: CLASSE PREDICTION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Add predicted CATEGORIA\n",
    "df_processed['CATEGORIA_encoded'] = pred_cat_encoded\n",
    "\n",
    "# Add missing columns for CLASSE\n",
    "for col in expected_features_classe:\n",
    "    if col not in df_processed.columns:\n",
    "        if col in df_model_reference.columns:\n",
    "            if pd.api.types.is_numeric_dtype(df_model_reference[col]):\n",
    "                df_processed[col] = df_model_reference[col].median()\n",
    "            else:\n",
    "                df_processed[col] = df_model_reference[col].mode()[0] if len(df_model_reference[col].mode()) > 0 else 0\n",
    "        else:\n",
    "            df_processed[col] = 0\n",
    "\n",
    "# Process categorical columns for CLASSE\n",
    "for col, le in label_encoders.items():\n",
    "    if col in expected_features_classe and col != 'CLASSE':\n",
    "        if col not in ['CATEGORIA_encoded']:\n",
    "            if col in df_processed.columns:\n",
    "                most_frequent = le.classes_[0]\n",
    "                df_processed[col] = df_processed[col].fillna(most_frequent).astype(str)\n",
    "                df_processed[col] = df_processed[col].apply(lambda x: x if x in le.classes_ else most_frequent)\n",
    "                df_processed[col] = le.transform(df_processed[col])\n",
    "\n",
    "X_classe = df_processed[expected_features_classe]\n",
    "\n",
    "pred_classe_encoded = model_classe.predict(X_classe)\n",
    "# Convert to int to avoid dtype casting errors\n",
    "pred_classe_encoded = pred_classe_encoded.astype(int)\n",
    "pred_classe = le_classe.inverse_transform(pred_classe_encoded)\n",
    "proba_classe = model_classe.predict_proba(X_classe)\n",
    "\n",
    "print(f\"\\nTop 3 CLASSE predictions:\")\n",
    "top3_classe_indices = np.argsort(proba_classe[0])[-3:][::-1]\n",
    "for i, idx in enumerate(top3_classe_indices, 1):\n",
    "    classe_name = le_classe.classes_[idx]\n",
    "    confidence = proba_classe[0][idx]\n",
    "    print(f\"  {i}. {classe_name}: {confidence:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# FINAL RESULTS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL PREDICTION\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nCATEGORIA: {pred_cat[0]} (confidence: {proba_cat[0][pred_cat_encoded[0]]:.4f})\")\n",
    "print(f\"CLASSE: {pred_classe[0]} (confidence: {proba_classe[0][pred_classe_encoded[0]]:.4f})\")\n",
    "print(f\"\\nFinal prediction: {pred_cat[0]}/{pred_classe[0]}\")\n",
    "\n",
    "\n",
    "\n",
    "# Save results\n",
    "risultato = pd.DataFrame({\n",
    "    'final_prediction': [f\"{pred_cat[0]}/{pred_classe[0]}\"],\n",
    "    'CATEGORIA_top1': [le_categoria.classes_[top3_cat_indices[0]]],\n",
    "    'CATEGORIA_top1_conf': [proba_cat[0][top3_cat_indices[0]]],\n",
    "    'CATEGORIA_top2': [le_categoria.classes_[top3_cat_indices[1]]],\n",
    "    'CATEGORIA_top2_conf': [proba_cat[0][top3_cat_indices[1]]],\n",
    "    'CATEGORIA_top3': [le_categoria.classes_[top3_cat_indices[2]]],\n",
    "    'CATEGORIA_top3_conf': [proba_cat[0][top3_cat_indices[2]]],\n",
    "    'CLASSE_top1': [le_classe.classes_[top3_classe_indices[0]]],\n",
    "    'CLASSE_top1_conf': [proba_classe[0][top3_classe_indices[0]]],\n",
    "    'CLASSE_top2': [le_classe.classes_[top3_classe_indices[1]]],\n",
    "    'CLASSE_top2_conf': [proba_classe[0][top3_classe_indices[1]]],\n",
    "    'CLASSE_top3': [le_classe.classes_[top3_classe_indices[2]]],\n",
    "    'CLASSE_top3_conf': [proba_classe[0][top3_classe_indices[2]]]\n",
    "})\n",
    "risultato.to_csv(\"inference_results/predictions_output.csv\", index=False)\n",
    "\n",
    "# Save SHAP analysis to CSV files\n",
    "shap_top30.to_csv(\"inference_results/shap_top30_features.csv\", index=False)\n",
    "shap_positive.to_csv(\"inference_results/shap_top10_positive.csv\", index=False)\n",
    "shap_negative.to_csv(\"inference_results/shap_top10_negative.csv\", index=False)\n",
    "\n",
    "print_report = False\n",
    "\n",
    "print(\"\\nResults saved:\")\n",
    "print(\"  - predictions_output.csv\")\n",
    "print(\"  - shap_top30_features.csv\")\n",
    "print(\"  - shap_top10_positive.csv (features that increased probability)\")\n",
    "print(\"  - shap_top10_negative.csv (features that decreased probability)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virt_env_312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
