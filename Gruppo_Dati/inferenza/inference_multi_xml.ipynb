{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# if inference_results directory does not exist, create it\n",
    "if not os.path.exists(\"inference_results\"):\n",
    "    os.makedirs(\"inference_results\")\n",
    "    \n",
    "from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Image\n",
    "from reportlab.lib.styles import getSampleStyleSheet\n",
    "from reportlab.lib.pagesizes import A4\n",
    "from reportlab.lib.units import cm\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/dec/miniconda3/envs/ai/bin/python\n",
      "['/home/dec/miniconda3/envs/ai/lib/python311.zip', '/home/dec/miniconda3/envs/ai/lib/python3.11', '/home/dec/miniconda3/envs/ai/lib/python3.11/lib-dynload', '', '/home/dec/miniconda3/envs/ai/lib/python3.11/site-packages', '/home/dec/aitomotive/segment-anything']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models and encoders loaded successfully\n",
      "CATEGORIA model features: 75\n",
      "CLASSE model features: 76\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# LOAD MODELS AND ENCODERS\n",
    "# ============================================================================\n",
    "DATA_DIR = \"data\"\n",
    "\n",
    "with open(os.path.join(DATA_DIR, \"model_categoria.pkl\"), \"rb\") as f:\n",
    "    model_categoria = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(DATA_DIR, \"model_classe.pkl\"), \"rb\") as f:\n",
    "    model_classe = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(DATA_DIR, \"label_encoders_final.pkl\"), \"rb\") as f:\n",
    "    label_encoders = pickle.load(f)\n",
    "\n",
    "le_categoria = label_encoders[\"CATEGORIA\"]\n",
    "le_classe = label_encoders[\"CLASSE\"]\n",
    "\n",
    "#df_model_reference = pd.read_csv(os.path.join(DATA_DIR, \"df_model.csv\"), low_memory=False)\n",
    "\n",
    "print(\"Models and encoders loaded successfully\")\n",
    "\n",
    "# Get expected features\n",
    "expected_features_cat = model_categoria.feature_names_in_\n",
    "expected_features_classe = model_classe.feature_names_in_\n",
    "\n",
    "print(f\"CATEGORIA model features: {len(expected_features_cat)}\")\n",
    "print(f\"CLASSE model features: {len(expected_features_classe)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CREATE WRAPPER FOR INPUT DATA\n",
    "# XML FILE -> DATAFRAME RECORD \n",
    "# Use it as guideline if input has different format\n",
    "# ============================================================================\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def parse_xml_input(xml_file):\n",
    "    \"\"\"\n",
    "    Parse a single XML file and extract apartment data.\n",
    "    Returns a DataFrame with the correct structure for the classifier.\n",
    "    \"\"\"\n",
    "    all_rows = []\n",
    "\n",
    "    try:\n",
    "        tree = ET.parse(xml_file)\n",
    "        root = tree.getroot()\n",
    "\n",
    "        filename = os.path.basename(xml_file)\n",
    "\n",
    "        elenco_ui = root.find('.//ElencoUI')\n",
    "        if elenco_ui is None:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        ui_elements = elenco_ui.findall('.//UICostituzione')\n",
    "\n",
    "        for ui in ui_elements:\n",
    "            row = {\"source_file\": filename}\n",
    "            # Identificativo Catastale PM\n",
    "            idpm = ui.find('.//ElencoIdentificativiCatastaliPM/IdentificativoCatastalePM')\n",
    "            if idpm is not None:\n",
    "                for k, v in idpm.attrib.items():\n",
    "                    row[k] = v\n",
    "\n",
    "            # Classamento\n",
    "            classamento = ui.find('.//Classamento')\n",
    "            if classamento is not None:\n",
    "                for k, v in classamento.attrib.items():\n",
    "                    row[k] = v\n",
    "\n",
    "            # Indirizzo\n",
    "            indir = ui.find('.//ElencoIndirizzi/Indirizzo')\n",
    "            if indir is not None:\n",
    "                for k, v in indir.attrib.items():\n",
    "                    row[k] = v\n",
    "\n",
    "            # Piani\n",
    "            piani = ui.findall('.//ElencoPiani/Piano')\n",
    "            row[\"lista_piani\"] = \";\".join(\n",
    "                [p.attrib.get(\"numeroPiano\", \"\") for p in piani]\n",
    "            )\n",
    "\n",
    "            # Mod1N-2\n",
    "            mod1n2 = ui.find('.//Mod1N-2')\n",
    "            if mod1n2 is not None:\n",
    "                for k, v in mod1n2.attrib.items():\n",
    "                    row[k] = v\n",
    "                for elem in mod1n2.iter():\n",
    "                    if elem is not mod1n2:\n",
    "                        for k, v in elem.attrib.items():\n",
    "                            row[k] = v\n",
    "            all_rows.append(row)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Errore parsing {xml_file}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df_xml = pd.DataFrame(all_rows)\n",
    "\n",
    "    # SAME post-processing as training\n",
    "    int_cols = ['comuneCatastale', 'foglio', 'numeratore', 'subalterno']\n",
    "    for col in int_cols:\n",
    "        if col in df_xml.columns:\n",
    "            df_xml[col] = (\n",
    "                df_xml[col]\n",
    "                .astype(str)\n",
    "                .str.extract(r\"(\\d+)\", expand=False)\n",
    "                .astype(float)\n",
    "            )\n",
    "\n",
    "    return df_xml\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# clean record\n",
    "# ============================================================================\n",
    "\n",
    "def clean_dataframe(df_final):\n",
    "    \"\"\"Clean the merged dataframe.\"\"\"\n",
    "    # Drop columns\n",
    "    columns_to_drop = ['source_file', 'codiceVia', 'indirizzoIT', 'civico1', 'civico2',\n",
    "                       'civico3', 'foglio', 'numeratore', 'subalterno','comuneCatastale',\n",
    "                       'estAltroDescrizione', 'intAltroDescrizione', 'altroDescrizione',\n",
    "                       'pavimentazioneAltroDescrizione', 'categoriaImmobiliare',\n",
    "                       'sottoCategoriaImmobiliare',\n",
    "                       ]\n",
    "\n",
    "    df_clean = df_final.drop(columns=columns_to_drop, axis=1, errors='ignore')\n",
    "    \n",
    "    return df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "xml_file = \"/home/dec/ai_challenge/new_version/preprocessed_1N_xml/preprocessed_xml/0_MUT_1383448_531871_NCA_250_406_2918_0_30_xml_anonymus.xml\"\n",
    "input_df = parse_xml_input(xml_file)\n",
    "refined_input_df = clean_dataframe(input_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Load column types\n",
    "with open(os.path.join(\"data/column_types.pkl\"), \"rb\") as f:\n",
    "    column_types = pickle.load(f)\n",
    "\n",
    "# Load medians of numerical columns\n",
    "medians_path = os.path.join(DATA_DIR, \"numeric_medians.pkl\")\n",
    "with open(medians_path, \"rb\") as f:\n",
    "    numeric_medians = pickle.load(f)\n",
    "\n",
    "required_features = set(model_categoria.feature_names_in_)\n",
    "filtered_column_types = {col: typ for col, typ in column_types.items() if col in required_features}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_xml_row(df_xml, filtered_column_types, label_encoders):\n",
    "    \"\"\"Prepare a single XML row DataFrame for the model (training-consistent).\"\"\"\n",
    "    \n",
    "    df_prepared = pd.DataFrame(index=[0])\n",
    "    \n",
    "    for col, col_type in filtered_column_types.items():\n",
    "        if col in df_xml.columns:\n",
    "            if col_type == \"boolean\":\n",
    "                df_prepared[col] = [1]\n",
    "\n",
    "            elif col_type == \"categorical\":\n",
    "                le = label_encoders.get(col)\n",
    "                if le is None:\n",
    "                    raise ValueError(f\"Missing LabelEncoder for column: {col}\")\n",
    "\n",
    "                val = df_xml[col].iloc[0]\n",
    "\n",
    "                if pd.isna(val):\n",
    "                    val = \"MISSING\"\n",
    "                else:\n",
    "                    val = str(val)\n",
    "\n",
    "                if val not in le.classes_:\n",
    "                    val = \"MISSING\"\n",
    "\n",
    "                df_prepared[col] = [le.transform([val])[0]]\n",
    "\n",
    "            else:  # numeric\n",
    "                val = df_xml[col].iloc[0]\n",
    "                try:\n",
    "                    df_prepared[col] = [float(val)]\n",
    "                except:\n",
    "                    val = numeric_medians.get(col, 0)  # mancante nel file â†’ metti mediana\n",
    "                    df_prepared[col] = [float(val)]\n",
    "\n",
    "        else:\n",
    "            if col_type == \"boolean\":\n",
    "                df_prepared[col] = [0]\n",
    "            elif col_type == \"numeric\":\n",
    "                df_prepared[col] = [numeric_medians.get(col, 0)]\n",
    "            else:  # categorical missing\n",
    "                le = label_encoders.get(col)\n",
    "                if le is None:\n",
    "                    raise ValueError(f\"Missing LabelEncoder for column: {col}\")\n",
    "\n",
    "                # no categorical -> MISSING\n",
    "                df_prepared[col] = [le.transform([\"MISSING\"])[0]]\n",
    "\n",
    "    return df_prepared\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_input = prepare_xml_row(refined_input_df, filtered_column_types, label_encoders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# PREPROCESSING\n",
    "# ============================================================================\n",
    "df_processed = df_input.copy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STEP 1: CATEGORIA PREDICTION\n",
      "======================================================================\n",
      "\n",
      "Top 3 CATEGORIA predictions:\n",
      "  1. C06: 0.7606\n",
      "  2. C02: 0.2393\n",
      "  3. A02: 0.0001\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# STEP 1: PREDICT CATEGORIA\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 1: CATEGORIA PREDICTION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "X_cat = df_processed[expected_features_cat]\n",
    "pred_cat_encoded = model_categoria.predict(X_cat)\n",
    "# Convert to int to avoid dtype casting errors\n",
    "pred_cat_encoded = pred_cat_encoded.astype(int)\n",
    "pred_cat = le_categoria.inverse_transform(pred_cat_encoded)\n",
    "proba_cat = model_categoria.predict_proba(X_cat)\n",
    "\n",
    "print(f\"\\nTop 3 CATEGORIA predictions:\")\n",
    "top3_cat_indices = np.argsort(proba_cat[0])[-3:][::-1]\n",
    "for i, idx in enumerate(top3_cat_indices, 1):\n",
    "    cat_name = le_categoria.classes_[idx]\n",
    "    confidence = proba_cat[0][idx]\n",
    "    print(f\"  {i}. {cat_name}: {confidence:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "SHAP ANALYSIS - CATEGORIA\n",
      "======================================================================\n",
      "Computing SHAP values...\n",
      "\n",
      "Top 30 most influential features for CATEGORIA=C06:\n",
      "----------------------------------------------------------------------\n",
      "  numeroPiano: -1.00 | SHAP: +0.0792 (positive)\n",
      "  superficieMq: 30.00 | SHAP: +0.0780 (positive)\n",
      "  estFinestreMetallo: 1.00 | SHAP: -0.0717 (negative)\n",
      "  superficieLordaMq: 34.00 | SHAP: +0.0672 (positive)\n",
      "  lista_piani: -1.00 | SHAP: +0.0615 (positive)\n",
      "  annoRiferimento: 2018.00 | SHAP: +0.0558 (positive)\n",
      "  cucinaBagnoPiastrelleCeramica: 0.00 | SHAP: +0.0545 (positive)\n",
      "  tipoRiferimento: 0.00 | SHAP: +0.0429 (positive)\n",
      "  acquaCalda: 0.00 | SHAP: +0.0335 (positive)\n",
      "  superficieUtileMq: 63.00 | SHAP: +0.0330 (positive)\n",
      "  bagniSuperficieUtileMq: 7.00 | SHAP: +0.0267 (positive)\n",
      "  altezzaMediaUtileCm: 265.00 | SHAP: +0.0254 (positive)\n",
      "  intPorteInterneLegnoTamburato: 0.00 | SHAP: +0.0238 (positive)\n",
      "  altezzaMediaLocaliPrincipaliCm: 250.00 | SHAP: +0.0228 (positive)\n",
      "  accessoCarrabile: 0.00 | SHAP: -0.0225 (negative)\n",
      "  riscaldamento: 0.00 | SHAP: +0.0221 (positive)\n",
      "  corridoiSuperficieUtileMq: 11.00 | SHAP: +0.0176 (positive)\n",
      "  altriAccessoriPiastrelleCeramica: 0.00 | SHAP: +0.0165 (positive)\n",
      "  bagniNum: 1.00 | SHAP: +0.0162 (positive)\n",
      "  intPorteIngressoLegnoMassello: 0.00 | SHAP: +0.0139 (positive)\n",
      "  intPorteIngressoMetallo: 0.00 | SHAP: -0.0135 (negative)\n",
      "  postoAutoScoperto: 0.00 | SHAP: -0.0117 (negative)\n",
      "  spessoreMuri: 1.00 | SHAP: +0.0102 (positive)\n",
      "  altriAccessoriAltro: 1.00 | SHAP: +0.0100 (positive)\n",
      "  denominatore: 1.00 | SHAP: +0.0097 (positive)\n",
      "  camereParquet: 0.00 | SHAP: +0.0082 (positive)\n",
      "  corridoiNum: 2.00 | SHAP: +0.0080 (positive)\n",
      "  camerePiastrelleCeramica: 0.00 | SHAP: +0.0076 (positive)\n",
      "  condizionamento: 0.00 | SHAP: +0.0072 (positive)\n",
      "  estFinestreLegnoMassello: 0.00 | SHAP: +0.0070 (positive)\n",
      "\n",
      "Top 10 features that INCREASED probability:\n",
      "----------------------------------------------------------------------\n",
      "  numeroPiano: -1.00 | SHAP: +0.0792\n",
      "  superficieMq: 30.00 | SHAP: +0.0780\n",
      "  superficieLordaMq: 34.00 | SHAP: +0.0672\n",
      "  lista_piani: -1.00 | SHAP: +0.0615\n",
      "  annoRiferimento: 2018.00 | SHAP: +0.0558\n",
      "  cucinaBagnoPiastrelleCeramica: 0.00 | SHAP: +0.0545\n",
      "  tipoRiferimento: 0.00 | SHAP: +0.0429\n",
      "  acquaCalda: 0.00 | SHAP: +0.0335\n",
      "  superficieUtileMq: 63.00 | SHAP: +0.0330\n",
      "  bagniSuperficieUtileMq: 7.00 | SHAP: +0.0267\n",
      "\n",
      "Top 10 features that DECREASED probability:\n",
      "----------------------------------------------------------------------\n",
      "  estFinestreMetallo: 1.00 | SHAP: -0.0717\n",
      "  accessoCarrabile: 0.00 | SHAP: -0.0225\n",
      "  intPorteIngressoMetallo: 0.00 | SHAP: -0.0135\n",
      "  postoAutoScoperto: 0.00 | SHAP: -0.0117\n",
      "  intPorteInterneMetallo: 0.00 | SHAP: -0.0007\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# SHAP EXPLAINABILITY - CATEGORIA\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SHAP ANALYSIS - CATEGORIA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"Computing SHAP values...\")\n",
    "explainer_cat = shap.TreeExplainer(model_categoria)\n",
    "shap_values_cat = explainer_cat.shap_values(X_cat)\n",
    "\n",
    "# Extract values for predicted class\n",
    "if isinstance(shap_values_cat, np.ndarray) and shap_values_cat.ndim == 3:\n",
    "    shap_values_cat_pred = shap_values_cat[0, :, pred_cat_encoded[0]]\n",
    "    base_value = explainer_cat.expected_value[pred_cat_encoded[0]]\n",
    "elif isinstance(shap_values_cat, list):\n",
    "    shap_values_cat_pred = shap_values_cat[pred_cat_encoded[0]][0]\n",
    "    base_value = explainer_cat.expected_value[pred_cat_encoded[0]]\n",
    "else:\n",
    "    shap_values_cat_pred = shap_values_cat[0]\n",
    "    base_value = explainer_cat.expected_value\n",
    "\n",
    "# Top 30 most influential features\n",
    "shap_importance_cat = pd.DataFrame({\n",
    "    'feature': expected_features_cat,\n",
    "    'shap_value': shap_values_cat_pred,\n",
    "    'feature_value': X_cat.iloc[0].values\n",
    "})\n",
    "shap_importance_cat['abs_shap'] = shap_importance_cat['shap_value'].abs()\n",
    "shap_top30 = shap_importance_cat.sort_values('abs_shap', ascending=False).head(30)\n",
    "\n",
    "# Top 10 features that increased probability (positive SHAP)\n",
    "shap_positive = shap_importance_cat[shap_importance_cat['shap_value'] > 0].sort_values('shap_value', ascending=False).head(10)\n",
    "\n",
    "# Top 10 features that decreased probability (negative SHAP)\n",
    "shap_negative = shap_importance_cat[shap_importance_cat['shap_value'] < 0].sort_values('shap_value', ascending=True).head(10)\n",
    "\n",
    "print(f\"\\nTop 30 most influential features for CATEGORIA={pred_cat[0]}:\")\n",
    "print(\"-\" * 70)\n",
    "for idx, row in shap_top30.iterrows():\n",
    "    direction = \"positive\" if row['shap_value'] > 0 else \"negative\"\n",
    "    print(f\"  {row['feature']}: {row['feature_value']:.2f} | SHAP: {row['shap_value']:+.4f} ({direction})\")\n",
    "\n",
    "print(f\"\\nTop 10 features that INCREASED probability:\")\n",
    "print(\"-\" * 70)\n",
    "for idx, row in shap_positive.iterrows():\n",
    "    print(f\"  {row['feature']}: {row['feature_value']:.2f} | SHAP: {row['shap_value']:+.4f}\")\n",
    "\n",
    "print(f\"\\nTop 10 features that DECREASED probability:\")\n",
    "print(\"-\" * 70)\n",
    "for idx, row in shap_negative.iterrows():\n",
    "    print(f\"  {row['feature']}: {row['feature_value']:.2f} | SHAP: {row['shap_value']:+.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STEP 2: CLASSE PREDICTION\n",
      "======================================================================\n",
      "\n",
      "Top 3 CLASSE predictions:\n",
      "  1. 4.0: 0.3080\n",
      "  2. 3.0: 0.2810\n",
      "  3. 2.0: 0.2416\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# STEP 2: PREDICT CLASSE\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 2: CLASSE PREDICTION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Add predicted CATEGORIA\n",
    "df_processed['CATEGORIA'] = pred_cat_encoded\n",
    "\n",
    "X_classe = df_processed[expected_features_classe]\n",
    "\n",
    "pred_classe_encoded = model_classe.predict(X_classe)\n",
    "# Convert to int to avoid dtype casting errors\n",
    "pred_classe_encoded = pred_classe_encoded.astype(int)\n",
    "pred_classe = le_classe.inverse_transform(pred_classe_encoded)\n",
    "proba_classe = model_classe.predict_proba(X_classe)\n",
    "\n",
    "print(f\"\\nTop 3 CLASSE predictions:\")\n",
    "top3_classe_indices = np.argsort(proba_classe[0])[-3:][::-1]\n",
    "for i, idx in enumerate(top3_classe_indices, 1):\n",
    "    classe_name = le_classe.classes_[idx]\n",
    "    confidence = proba_classe[0][idx]\n",
    "    print(f\"  {i}. {classe_name}: {confidence:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "FINAL PREDICTION\n",
      "======================================================================\n",
      "\n",
      "CATEGORIA: C06 (confidence: 0.7606)\n",
      "CLASSE: 4.0 (confidence: 0.3080)\n",
      "\n",
      "Final prediction: C06/4.0\n",
      "\n",
      "Results saved:\n",
      "  - predictions_output.csv\n",
      "  - shap_top30_features.csv\n",
      "  - shap_top10_positive.csv (features that increased probability)\n",
      "  - shap_top10_negative.csv (features that decreased probability)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# FINAL RESULTS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL PREDICTION\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nCATEGORIA: {pred_cat[0]} (confidence: {proba_cat[0][pred_cat_encoded[0]]:.4f})\")\n",
    "print(f\"CLASSE: {pred_classe[0]} (confidence: {proba_classe[0][pred_classe_encoded[0]]:.4f})\")\n",
    "print(f\"\\nFinal prediction: {pred_cat[0]}/{pred_classe[0]}\")\n",
    "\n",
    "\n",
    "\n",
    "# Save results\n",
    "risultato = pd.DataFrame({\n",
    "    'final_prediction': [f\"{pred_cat[0]}/{pred_classe[0]}\"],\n",
    "    'CATEGORIA_top1': [le_categoria.classes_[top3_cat_indices[0]]],\n",
    "    'CATEGORIA_top1_conf': [proba_cat[0][top3_cat_indices[0]]],\n",
    "    'CATEGORIA_top2': [le_categoria.classes_[top3_cat_indices[1]]],\n",
    "    'CATEGORIA_top2_conf': [proba_cat[0][top3_cat_indices[1]]],\n",
    "    'CATEGORIA_top3': [le_categoria.classes_[top3_cat_indices[2]]],\n",
    "    'CATEGORIA_top3_conf': [proba_cat[0][top3_cat_indices[2]]],\n",
    "    'CLASSE_top1': [le_classe.classes_[top3_classe_indices[0]]],\n",
    "    'CLASSE_top1_conf': [proba_classe[0][top3_classe_indices[0]]],\n",
    "    'CLASSE_top2': [le_classe.classes_[top3_classe_indices[1]]],\n",
    "    'CLASSE_top2_conf': [proba_classe[0][top3_classe_indices[1]]],\n",
    "    'CLASSE_top3': [le_classe.classes_[top3_classe_indices[2]]],\n",
    "    'CLASSE_top3_conf': [proba_classe[0][top3_classe_indices[2]]]\n",
    "})\n",
    "risultato.to_csv(\"inference_results/predictions_output.csv\", index=False)\n",
    "\n",
    "# Save SHAP analysis to CSV files\n",
    "shap_top30.to_csv(\"inference_results/shap_top30_features.csv\", index=False)\n",
    "shap_positive.to_csv(\"inference_results/shap_top10_positive.csv\", index=False)\n",
    "shap_negative.to_csv(\"inference_results/shap_top10_negative.csv\", index=False)\n",
    "\n",
    "print_report = False\n",
    "\n",
    "print(\"\\nResults saved:\")\n",
    "print(\"  - predictions_output.csv\")\n",
    "print(\"  - shap_top30_features.csv\")\n",
    "print(\"  - shap_top10_positive.csv (features that increased probability)\")\n",
    "print(\"  - shap_top10_negative.csv (features that decreased probability)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
